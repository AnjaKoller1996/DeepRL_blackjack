##Oiginal Sourcecode From: https://github.com/Pradhyo/blackjack/blob/master/blackjack.ipynb

import gym
import matplotlib.pyplot as plt
from gym import wrappers
import random
import pandas as pd
#from pathlib import Path

#data_folder = Path("C:/Users/sebas/Desktop/DRL")

class Agent():
    def __init__(self, env, epsilon, alpha, gamma, num_episodes_to_train):
        self.env = env

        # Looks like n is number of valid actions from the souce code
        self.valid_actions = list(range(self.env.action_space.n))

        # Set parameters of the learning agent
        self.Q = dict()          # Q-table which will be a dictionary of tuples
        self.epsilon = epsilon   # Random exploration factor
        self.alpha = alpha       # Learning factor
        self.gamma = gamma       # Discount factor- closer to 1 learns well into distant future

        # epsilon will reduce linearly until it reaches 0 based on num_episodes_to_train
        # epsilon drops to 90% of its inital value in the first 30% of num_episodes_to_train
        # epsilon then drops to 10% of its initial value in the next 40% of num_episodes_to_train
        # epsilon finally becomes 0 in the final 30% of num_episodes_to_train
        self.num_episodes_to_train = num_episodes_to_train # Change epsilon each episode based on this
        self.small_decrement = (0.1 * epsilon) / (0.3 * num_episodes_to_train) # reduces epsilon slowly
        self.big_decrement = (0.8 * epsilon) / (0.4 * num_episodes_to_train) # reduces epilon faster

        self.num_episodes_to_train_left = num_episodes_to_train

    def update_parameters(self):
        """
        Update epsilon and alpha after each action
        Set them to 0 if not learning
        """
        if self.num_episodes_to_train_left > 0.7 * self.num_episodes_to_train:
            self.epsilon -= self.small_decrement
        elif self.num_episodes_to_train_left > 0.3 * self.num_episodes_to_train:
            self.epsilon -= self.big_decrement
        elif self.num_episodes_to_train_left > 0:
            self.epsilon -= self.small_decrement
        else:
            self.epsilon = 0.0
            self.alpha = 0.0

        self.num_episodes_to_train_left -= 1

    def create_Q_if_new_observation(self, observation):
        """
        Set intial Q values to 0.0 if observation not already in Q table
        """
        if observation not in self.Q:
            self.Q[observation] = dict((action, 0.0) for action in self.valid_actions)

    def get_maxQ(self, observation):
        """
        Called when the agent is asked to find the maximum Q-value of
        all actions based on the 'observation' the environment is in.
        """
        self.create_Q_if_new_observation(observation)
        return max(self.Q[observation].values())

    def choose_action(self, observation):
        """
        Choose which action to take, based on the observation.
        If observation is seen for the first time, initialize its Q values to 0.0
        """
        self.create_Q_if_new_observation(observation)

        # uniformly distributed random number > epsilon happens with probability 1-epsilon
        if random.random() > self.epsilon:
            maxQ = self.get_maxQ(observation)
            #print(maxQ)
            # multiple actions could have maxQ- pick one at random in that case
            # this is also the case when the Q value for this observation were just set to 0.0
            action = random.choice([k for k in self.Q[observation].keys()
                                    if self.Q[observation][k] == maxQ])
        else:
            action = random.choice(self.valid_actions)
            #print("random")
        self.update_parameters()
        #print("updated")
        return action


    def learn(self, observation, action, reward, next_observation):
        """
        Called after the agent completes an action and receives an award.
        This function does not consider future rewards
        when conducting learning.
        """

        # Q = Q*(1-alpha) + alpha(reward + discount * utility of next observation)
        # Q = Q - Q * alpha + alpha(reward + discount * self.get_maxQ(next_observation))
        # Q = Q - alpha (-Q + reward + discount * self.get_maxQ(next_observation))
        self.Q[observation][action] += self.alpha * (reward
                                                     + (self.gamma * self.get_maxQ(next_observation))
                                                     - self.Q[observation][action])


env = gym.make('Blackjack-v0')


agent = Agent(env=env, epsilon=1.0, alpha= 1, gamma= 1, num_episodes_to_train=800000)

num_rounds = 1000  # Payout calculated over num_rounds
num_samples = 1000  # num_rounds simulated over num_samples

average_payouts = []
payout_list = []
observation = env.reset()
for sample in range(num_samples):
    round = 1
    total_payout = 0  # to store total payout over 'num_rounds'
    # Take action based on Q-table of the agent and learn based on that until 'num_episodes_to_train' = 0
    while round <= num_rounds:
        action = agent.choose_action(observation)
        next_observation, payout, is_done, _ = env.step(action)
        agent.learn(observation, action, payout, next_observation)
        total_payout += payout

        observation = next_observation
        if is_done:
            payout_list.append(payout)
            observation = env.reset()  # Environment deals new cards to player and dealer
            round += 1
    average_payouts.append(total_payout)

#print("Average payout after {} rounds is {}".format(num_rounds, sum(average_payouts) / (num_samples)))
#print(sum(average_payouts[900:999])/len(average_payouts[900:999]))

print(payout_list[-1000:])
payout_list_last = payout_list[-100000:]
winning = payout_list_last.count(1)/len(payout_list_last)
drawing = payout_list_last.count(0)/len(payout_list_last)
loosing = payout_list_last.count(-1)/len(payout_list_last)
natural = payout_list_last.count(1.5)/len(payout_list_last)

print("winnin",winning,"drawing",drawing,"loosing",loosing, "natural",natural)


#df = pd.DataFrame(average_payouts, columns= ['Gamma 0.8'])
#print (average_payouts)
#export_csv = df.to_csv (r'C:/Users/sebas/Desktop/DRL/Qlearning_a0.5_g0.8.csv', index = None, header=True)
